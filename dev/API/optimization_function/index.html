<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>OptimizationFunction · Optimization.jl</title><meta name="title" content="OptimizationFunction · Optimization.jl"/><meta property="og:title" content="OptimizationFunction · Optimization.jl"/><meta property="twitter:title" content="OptimizationFunction · Optimization.jl"/><meta name="description" content="Documentation for Optimization.jl."/><meta property="og:description" content="Documentation for Optimization.jl."/><meta property="twitter:description" content="Documentation for Optimization.jl."/><meta property="og:url" content="https://docs.sciml.ai/Optimization/stable/API/optimization_function/"/><meta property="twitter:url" content="https://docs.sciml.ai/Optimization/stable/API/optimization_function/"/><link rel="canonical" href="https://docs.sciml.ai/Optimization/stable/API/optimization_function/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Optimization.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Optimization.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Optimization.jl: A Unified Optimization Package</a></li><li><a class="tocitem" href="../../getting_started/">Getting Started with Optimization in Julia</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/minibatch/">Data Iterators and Minibatching</a></li><li><a class="tocitem" href="../../tutorials/symbolic/">Symbolic Problem Building with ModelingToolkit</a></li><li><a class="tocitem" href="../../tutorials/constraints/">Using Equality and Inequality Constraints</a></li><li><a class="tocitem" href="../../tutorials/linearandinteger/">Linear and Integer Optimization Problems</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../../examples/rosenbrock/">Solving the Rosenbrock Problem in &gt;10 Ways</a></li></ul></li><li><span class="tocitem">Basics</span><ul><li><a class="tocitem" href="../optimization_problem/">Defining OptimizationProblems</a></li><li class="is-active"><a class="tocitem" href>OptimizationFunction</a><ul class="internal"><li><a class="tocitem" href="#Automatic-Differentiation-Construction-Choice-Recommendations"><span>Automatic Differentiation Construction Choice Recommendations</span></a></li><li><a class="tocitem" href="#Automatic-Differentiation-Choice-API"><span>Automatic Differentiation Choice API</span></a></li></ul></li><li><a class="tocitem" href="../solve/">Common Solver Options (Solve Keyword Arguments)</a></li><li><a class="tocitem" href="../optimization_solution/">Optimization Solutions</a></li><li><a class="tocitem" href="../modelingtoolkit/">ModelingToolkit Integration</a></li><li><a class="tocitem" href="../FAQ/">Frequently Asked Questions</a></li></ul></li><li><span class="tocitem">Optimizer Packages</span><ul><li><a class="tocitem" href="../../optimization_packages/blackboxoptim/">BlackBoxOptim.jl</a></li><li><a class="tocitem" href="../../optimization_packages/cmaevolutionstrategy/">CMAEvolutionStrategy.jl</a></li><li><a class="tocitem" href="../../optimization_packages/evolutionary/">Evolutionary.jl</a></li><li><a class="tocitem" href="../../optimization_packages/flux/">Flux.jl</a></li><li><a class="tocitem" href="../../optimization_packages/gcmaes/">GCMAES.jl</a></li><li><a class="tocitem" href="../../optimization_packages/mathoptinterface/">MathOptInterface.jl</a></li><li><a class="tocitem" href="../../optimization_packages/metaheuristics/">Metaheuristics.jl</a></li><li><a class="tocitem" href="../../optimization_packages/multistartoptimization/">MultistartOptimization.jl</a></li><li><a class="tocitem" href="../../optimization_packages/nlopt/">NLopt.jl</a></li><li><a class="tocitem" href="../../optimization_packages/nomad/">NOMAD.jl</a></li><li><a class="tocitem" href="../../optimization_packages/optim/">Optim.jl</a></li><li><a class="tocitem" href="../../optimization_packages/optimisers/">Optimisers.jl</a></li><li><a class="tocitem" href="../../optimization_packages/prima/">PRIMA.jl</a></li><li><a class="tocitem" href="../../optimization_packages/polyopt/">Polyalgorithms.jl</a></li><li><a class="tocitem" href="../../optimization_packages/quaddirect/">QuadDIRECT.jl</a></li><li><a class="tocitem" href="../../optimization_packages/speedmapping/">SpeedMapping.jl</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Basics</a></li><li class="is-active"><a href>OptimizationFunction</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>OptimizationFunction</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/Optimization.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/Optimization.jl/blob/master/docs/src/API/optimization_function.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="optfunction"><a class="docs-heading-anchor" href="#optfunction">OptimizationFunction</a><a id="optfunction-1"></a><a class="docs-heading-anchor-permalink" href="#optfunction" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="SciMLBase.OptimizationFunction" href="#SciMLBase.OptimizationFunction"><code>SciMLBase.OptimizationFunction</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct OptimizationFunction{iip, AD, F, G, H, HV, C, CJ, CH, HP, CJP, CHP, O, EX, CEX, SYS, LH, LHP, HCV, CJCV, CHCV, LHCV} &lt;: SciMLBase.AbstractOptimizationFunction{iip}</code></pre><p>A representation of an objective function <code>f</code>, defined by:</p><p class="math-container">\[\min_{u} f(u,p)\]</p><p>and all of its related functions, such as the gradient of <code>f</code>, its Hessian, and more. For all cases, <code>u</code> is the state and <code>p</code> are the parameters.</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">OptimizationFunction{iip}(f, adtype::AbstractADType = NoAD();
                          grad = nothing, hess = nothing, hv = nothing,
                          cons = nothing, cons_j = nothing, cons_h = nothing,
                          hess_prototype = nothing,
                          cons_jac_prototype = nothing,
                          cons_hess_prototype = nothing,
                          syms = nothing,
                          paramsyms = nothing,
                          observed = __has_observed(f) ? f.observed : DEFAULT_OBSERVED_NO_TIME,
                          lag_h = nothing,
                          hess_colorvec = __has_colorvec(f) ? f.colorvec : nothing,
                          cons_jac_colorvec = __has_colorvec(f) ? f.colorvec : nothing,
                          cons_hess_colorvec = __has_colorvec(f) ? f.colorvec : nothing,
                          lag_hess_colorvec = nothing,
                          sys = __has_sys(f) ? f.sys : nothing)</code></pre><p><strong>Positional Arguments</strong></p><ul><li><code>f(u,p,args...)</code>: the function to optimize. <code>u</code> are the optimization variables and <code>p</code> are parameters used in definition of</li></ul><p>the objective, even if no such parameters are used in the objective it should be an argument in the function. This can also take any additional arguments that are relevant to the objective function, for example minibatches used in machine learning, take a look at the minibatching tutorial <a href="https://docs.sciml.ai/Optimization/stable/tutorials/minibatch/">here</a>. This should return a scalar, the loss value, as the first return output and if any additional outputs are returned, they will be passed to the <code>callback</code> function described in <a href="https://docs.sciml.ai/Optimization/stable/API/solve/#Common-Solver-Options-(Solve-Keyword-Arguments)">Callback Functions</a>.</p><ul><li><code>adtype</code>: see the Defining Optimization Functions via AD section below.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>grad(G,u,p)</code> or <code>G=grad(u,p)</code>: the gradient of <code>f</code> with respect to <code>u</code>. If <code>f</code> takes additional arguments   then <code>grad(G,u,p,args...)</code> or <code>G=grad(u,p,args...)</code> should be used.</li><li><code>hess(H,u,p)</code> or <code>H=hess(u,p)</code>: the Hessian of <code>f</code> with respect to <code>u</code>. If <code>f</code> takes additional arguments   then <code>hess(H,u,p,args...)</code> or <code>H=hess(u,p,args...)</code> should be used.</li><li><code>hv(Hv,u,v,p)</code> or <code>Hv=hv(u,v,p)</code>: the Hessian-vector product <span>$(d^2 f / du^2) v$</span>. If <code>f</code> takes additional arguments   then <code>hv(Hv,u,v,p,args...)</code> or <code>Hv=hv(u,v,p, args...)</code> should be used.</li><li><code>cons(res,x,p)</code> or <code>res=cons(x,p)</code> : the constraints function, should mutate the passed <code>res</code> array   with value of the <code>i</code>th constraint, evaluated at the current values of variables   inside the optimization routine. This takes just the function evaluations   and the equality or inequality assertion is applied by the solver based on the constraint   bounds passed as <code>lcons</code> and <code>ucons</code> to <a href="../optimization_problem/#SciMLBase.OptimizationProblem"><code>OptimizationProblem</code></a>, in case of equality   constraints <code>lcons</code> and <code>ucons</code> should be passed equal values.</li><li><code>cons_j(J,x,p)</code> or <code>J=cons_j(x,p)</code>: the Jacobian of the constraints.</li><li><code>cons_h(H,x,p)</code> or <code>H=cons_h(x,p)</code>: the Hessian of the constraints, provided as  an array of Hessians with <code>res[i]</code> being the Hessian with respect to the <code>i</code>th output on <code>cons</code>.</li><li><code>hess_prototype</code>: a prototype matrix matching the type that matches the Hessian. For example, if the Hessian is tridiagonal, then an appropriately sized <code>Hessian</code> matrix can be used as the prototype and optimization solvers will specialize on this structure where possible. Non-structured sparsity patterns should use a <code>SparseMatrixCSC</code> with a correct sparsity pattern for the Hessian. The default is <code>nothing</code>, which means a dense Hessian.</li><li><code>cons_jac_prototype</code>: a prototype matrix matching the type that matches the constraint Jacobian. The default is <code>nothing</code>, which means a dense constraint Jacobian.</li><li><code>cons_hess_prototype</code>: a prototype matrix matching the type that matches the constraint Hessian. This is defined as an array of matrices, where <code>hess[i]</code> is the Hessian w.r.t. the <code>i</code>th output. For example, if the Hessian is sparse, then <code>hess</code> is a <code>Vector{SparseMatrixCSC}</code>. The default is <code>nothing</code>, which means a dense constraint Hessian.</li><li><code>lag_h(res,x,sigma,mu,p)</code> or <code>res=lag_h(x,sigma,mu,p)</code>: the Hessian of the Lagrangian, where <code>sigma</code> is a multiplier of the cost function and <code>mu</code> are the Lagrange multipliers multiplying the constraints. This can be provided instead of <code>hess</code> and <code>cons_h</code> to solvers that directly use the Hessian of the Lagrangian.</li><li><code>hess_colorvec</code>: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the <code>hess_prototype</code>. This specializes the Hessian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to <code>nothing</code>, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.</li><li><code>cons_jac_colorvec</code>: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the <code>cons_jac_prototype</code>.</li><li><code>cons_hess_colorvec</code>: an array of color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the <code>cons_hess_prototype</code>.</li></ul><p>When <a href="https://docs.sciml.ai/Optimization/stable/tutorials/symbolic/">Symbolic Problem Building with ModelingToolkit</a> interface is used the following arguments are also relevant:</p><ul><li><code>syms</code>: the symbol names for the elements of the equation. This should match <code>u0</code> in size. For example, if <code>u = [0.0,1.0]</code> and <code>syms = [:x, :y]</code>, this will apply a canonical naming to the values, allowing <code>sol[:x]</code> in the solution and automatically naming values in plots.</li><li><code>paramsyms</code>: the symbol names for the parameters of the equation. This should match <code>p</code> in size. For example, if <code>p = [0.0, 1.0]</code> and <code>paramsyms = [:a, :b]</code>, this will apply a canonical naming to the values, allowing <code>sol[:a]</code> in the solution.</li><li><code>observed</code>: an algebraic combination of optimization variables that is of interest to the user   which will be available in the solution. This can be single or multiple expressions.</li><li><code>sys</code>: field that stores the <code>OptimizationSystem</code>.</li></ul><p><strong>Defining Optimization Functions via AD</strong></p><p>While using the keyword arguments gives the user control over defining all of the possible functions, the simplest way to handle the generation of an <code>OptimizationFunction</code> is by specifying the <code>ADtype</code> which lets the user choose the Automatic Differentiation backend to use for automatically filling in all of the extra functions. For example,</p><pre><code class="language-julia hljs">OptimizationFunction(f,AutoForwardDiff())</code></pre><p>will use <a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff.jl</a> to define all of the necessary functions. Note that if any functions are defined directly, the auto-AD definition does not overwrite the user&#39;s choice.</p><p>Each of the AD-based constructors are documented separately via their own dispatches below in the <a href="#Automatic-Differentiation-Construction-Choice-Recommendations">Automatic Differentiation Construction Choice Recommendations</a> section.</p><p><strong>iip: In-Place vs Out-Of-Place</strong></p><p>For more details on this argument, see the ODEFunction documentation.</p><p><strong>specialize: Controlling Compilation and Specialization</strong></p><p>For more details on this argument, see the ODEFunction documentation.</p><p><strong>Fields</strong></p><p>The fields of the OptimizationFunction type directly match the names of the inputs.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/SciMLBase.jl/blob/v2.19.0/src/scimlfunctions.jl#L1952">source</a></section></article><h2 id="Automatic-Differentiation-Construction-Choice-Recommendations"><a class="docs-heading-anchor" href="#Automatic-Differentiation-Construction-Choice-Recommendations">Automatic Differentiation Construction Choice Recommendations</a><a id="Automatic-Differentiation-Construction-Choice-Recommendations-1"></a><a class="docs-heading-anchor-permalink" href="#Automatic-Differentiation-Construction-Choice-Recommendations" title="Permalink"></a></h2><p>The choices for the auto-AD fill-ins with quick descriptions are:</p><ul><li><code>AutoForwardDiff()</code>: The fastest choice for small optimizations</li><li><code>AutoReverseDiff(compile=false)</code>: A fast choice for large scalar optimizations</li><li><code>AutoTracker()</code>: Like ReverseDiff but GPU-compatible</li><li><code>AutoZygote()</code>: The fastest choice for non-mutating array-based (BLAS) functions</li><li><code>AutoFiniteDiff()</code>: Finite differencing, not optimal but always applicable</li><li><code>AutoModelingToolkit()</code>: The fastest choice for large scalar optimizations</li><li><code>AutoEnzyme()</code>: Highly performant AD choice for type stable and optimized code</li></ul><h2 id="Automatic-Differentiation-Choice-API"><a class="docs-heading-anchor" href="#Automatic-Differentiation-Choice-API">Automatic Differentiation Choice API</a><a id="Automatic-Differentiation-Choice-API-1"></a><a class="docs-heading-anchor-permalink" href="#Automatic-Differentiation-Choice-API" title="Permalink"></a></h2><p>The following sections describe the Auto-AD choices in detail.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ADTypes.AutoForwardDiff" href="#ADTypes.AutoForwardDiff"><code>ADTypes.AutoForwardDiff</code></a> — <span class="docstring-category">Type</span></header><section><div><p>AutoForwardDiff{chunksize} &lt;: AbstractADType</p><p>An AbstractADType choice for use in OptimizationFunction for automatically generating the unspecified derivative functions. Usage:</p><pre><code class="language-julia hljs">OptimizationFunction(f, AutoForwardDiff(); kwargs...)</code></pre><p>This uses the <a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff.jl</a> package. It is the fastest choice for small systems, especially with heavy scalar interactions. It is easy to use and compatible with most Julia functions which have loose type restrictions. However, because it&#39;s forward-mode, it scales poorly in comparison to other AD choices. Hessian construction is suboptimal as it uses the forward-over-forward approach.</p><ul><li>Compatible with GPUs</li><li>Compatible with Hessian-based optimization</li><li>Compatible with Hv-based optimization</li><li>Compatible with constraints</li></ul><p>Note that only the unspecified derivative functions are defined. For example, if a <code>hess</code> function is supplied to the <code>OptimizationFunction</code>, then the Hessian is not defined via ForwardDiff.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/Optimization.jl/blob/7a8298780910a58aec893898c88b78c32cca0010/src/adtypes.jl#L62-L88">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ADTypes.AutoFiniteDiff" href="#ADTypes.AutoFiniteDiff"><code>ADTypes.AutoFiniteDiff</code></a> — <span class="docstring-category">Type</span></header><section><div><p>AutoFiniteDiff{T1,T2,T3} &lt;: AbstractADType</p><p>An AbstractADType choice for use in OptimizationFunction for automatically generating the unspecified derivative functions. Usage:</p><pre><code class="language-julia hljs">OptimizationFunction(f, AutoFiniteDiff(); kwargs...)</code></pre><p>This uses <a href="https://github.com/JuliaDiff/FiniteDiff.jl">FiniteDiff.jl</a>. While not necessarily the most efficient, this is the only choice that doesn&#39;t require the <code>f</code> function to be automatically differentiable, which means it applies to any choice. However, because it&#39;s using finite differencing, one needs to be careful as this procedure introduces numerical error into the derivative estimates.</p><ul><li>Compatible with GPUs</li><li>Compatible with Hessian-based optimization</li><li>Compatible with Hv-based optimization</li><li>Compatible with constraint functions</li></ul><p>Note that only the unspecified derivative functions are defined. For example, if a <code>hess</code> function is supplied to the <code>OptimizationFunction</code>, then the Hessian is not defined via FiniteDiff.</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">AutoFiniteDiff(; fdtype = Val(:forward)fdjtype = fdtype, fdhtype = Val(:hcentral))</code></pre><ul><li><code>fdtype</code>: the method used for defining the gradient</li><li><code>fdjtype</code>: the method used for defining the Jacobian of constraints.</li><li><code>fdhtype</code>: the method used for defining the Hessian</li></ul><p>For more information on the derivative type specifiers, see the <a href="https://github.com/JuliaDiff/FiniteDiff.jl">FiniteDiff.jl documentation</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/Optimization.jl/blob/7a8298780910a58aec893898c88b78c32cca0010/src/adtypes.jl#L21-L59">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ADTypes.AutoReverseDiff" href="#ADTypes.AutoReverseDiff"><code>ADTypes.AutoReverseDiff</code></a> — <span class="docstring-category">Type</span></header><section><div><p>AutoReverseDiff &lt;: AbstractADType</p><p>An AbstractADType choice for use in OptimizationFunction for automatically generating the unspecified derivative functions. Usage:</p><pre><code class="language-julia hljs">OptimizationFunction(f, AutoReverseDiff(); kwargs...)</code></pre><p>This uses the <a href="https://github.com/JuliaDiff/ReverseDiff.jl">ReverseDiff.jl</a> package. <code>AutoReverseDiff</code> has a default argument, <code>compile</code>, which denotes whether the reverse pass should be compiled. <strong><code>compile</code> should only be set to <code>true</code> if <code>f</code> contains no branches (if statements, while loops) otherwise it can produce incorrect derivatives!</strong></p><p><code>AutoReverseDiff</code> is generally applicable to many pure Julia codes, and with <code>compile=true</code> it is one of the fastest options on code with heavy scalar interactions. Hessian calculations are fast by mixing ForwardDiff with ReverseDiff for forward-over-reverse. However, its performance can falter when <code>compile=false</code>.</p><ul><li>Not compatible with GPUs</li><li>Compatible with Hessian-based optimization by mixing with ForwardDiff</li><li>Compatible with Hv-based optimization by mixing with ForwardDiff</li><li>Not compatible with constraint functions</li></ul><p>Note that only the unspecified derivative functions are defined. For example, if a <code>hess</code> function is supplied to the <code>OptimizationFunction</code>, then the Hessian is not defined via ReverseDiff.</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">AutoReverseDiff(; compile = false)</code></pre><p><strong>Note: currently, compilation is not defined/used!</strong></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/Optimization.jl/blob/7a8298780910a58aec893898c88b78c32cca0010/src/adtypes.jl#L130-L168">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ADTypes.AutoZygote" href="#ADTypes.AutoZygote"><code>ADTypes.AutoZygote</code></a> — <span class="docstring-category">Type</span></header><section><div><p>AutoZygote &lt;: AbstractADType</p><p>An AbstractADType choice for use in OptimizationFunction for automatically generating the unspecified derivative functions. Usage:</p><pre><code class="language-julia hljs">OptimizationFunction(f, AutoZygote(); kwargs...)</code></pre><p>This uses the <a href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a> package. This is the staple reverse-mode AD that handles a large portion of Julia with good efficiency. Hessian construction is fast via forward-over-reverse mixing ForwardDiff.jl with Zygote.jl</p><ul><li>Compatible with GPUs</li><li>Compatible with Hessian-based optimization via ForwardDiff</li><li>Compatible with Hv-based optimization via ForwardDiff</li><li>Not compatible with constraint functions</li></ul><p>Note that only the unspecified derivative functions are defined. For example, if a <code>hess</code> function is supplied to the <code>OptimizationFunction</code>, then the Hessian is not defined via Zygote.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/Optimization.jl/blob/7a8298780910a58aec893898c88b78c32cca0010/src/adtypes.jl#L196-L219">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ADTypes.AutoTracker" href="#ADTypes.AutoTracker"><code>ADTypes.AutoTracker</code></a> — <span class="docstring-category">Type</span></header><section><div><p>AutoTracker &lt;: AbstractADType</p><p>An AbstractADType choice for use in OptimizationFunction for automatically generating the unspecified derivative functions. Usage:</p><pre><code class="language-julia hljs">OptimizationFunction(f, AutoTracker(); kwargs...)</code></pre><p>This uses the <a href="https://github.com/FluxML/Tracker.jl">Tracker.jl</a> package. Generally slower than ReverseDiff, it is generally applicable to many pure Julia codes.</p><ul><li>Compatible with GPUs</li><li>Not compatible with Hessian-based optimization</li><li>Not compatible with Hv-based optimization</li><li>Not compatible with constraint functions</li></ul><p>Note that only the unspecified derivative functions are defined. For example, if a <code>hess</code> function is supplied to the <code>OptimizationFunction</code>, then the Hessian is not defined via Tracker.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/Optimization.jl/blob/7a8298780910a58aec893898c88b78c32cca0010/src/adtypes.jl#L171-L193">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ADTypes.AutoModelingToolkit" href="#ADTypes.AutoModelingToolkit"><code>ADTypes.AutoModelingToolkit</code></a> — <span class="docstring-category">Type</span></header><section><div><p>AutoModelingToolkit &lt;: AbstractADType</p><p>An AbstractADType choice for use in OptimizationFunction for automatically generating the unspecified derivative functions. Usage:</p><pre><code class="language-julia hljs">OptimizationFunction(f, AutoModelingToolkit(); kwargs...)</code></pre><p>This uses the <a href="https://github.com/SciML/ModelingToolkit.jl">ModelingToolkit.jl</a> package&#39;s <code>modelingtookitize</code> functionality to generate the derivatives and other fields of an <code>OptimizationFunction</code>. This backend creates the symbolic expressions for the objective and its derivatives as well as the constraints and their derivatives. Through <code>structural_simplify</code>, it enforces simplifications that can reduce the number of operations needed to compute the derivatives of the constraints. This automatically generates the expression graphs that some solver interfaces through OptimizationMOI like <a href="https://github.com/jump-dev/AmplNLWriter.jl">AmplNLWriter.jl</a> require.</p><ul><li>Compatible with GPUs</li><li>Compatible with Hessian-based optimization</li><li>Compatible with Hv-based optimization</li><li>Compatible with constraints</li></ul><p>Note that only the unspecified derivative functions are defined. For example, if a <code>hess</code> function is supplied to the <code>OptimizationFunction</code>, then the Hessian is not generated via ModelingToolkit.</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">AutoModelingToolkit(false, false)</code></pre><ul><li><code>obj_sparse</code>: to indicate whether the objective hessian is sparse.</li><li><code>cons_sparse</code>: to indicate whether the constraints&#39; jacobian and hessian are sparse.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/Optimization.jl/blob/7a8298780910a58aec893898c88b78c32cca0010/src/adtypes.jl#L91-L127">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="ADTypes.AutoEnzyme" href="#ADTypes.AutoEnzyme"><code>ADTypes.AutoEnzyme</code></a> — <span class="docstring-category">Type</span></header><section><div><p>AutoEnzyme &lt;: AbstractADType</p><p>An AbstractADType choice for use in OptimizationFunction for automatically generating the unspecified derivative functions. Usage:</p><pre><code class="language-julia hljs">OptimizationFunction(f, AutoEnzyme(); kwargs...)</code></pre><p>This uses the <a href="https://github.com/EnzymeAD/Enzyme.jl">Enzyme.jl</a> package. Enzyme performs automatic differentiation on the LLVM IR code generated from julia. It is highly-efficient and its ability perform AD on optimized code allows Enzyme to meet or exceed the performance of state-of-the-art AD tools.</p><ul><li>Compatible with GPUs</li><li>Compatible with Hessian-based optimization</li><li>Compatible with Hv-based optimization</li><li>Compatible with constraints</li></ul><p>Note that only the unspecified derivative functions are defined. For example, if a <code>hess</code> function is supplied to the <code>OptimizationFunction</code>, then the Hessian is not defined via Enzyme.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/Optimization.jl/blob/7a8298780910a58aec893898c88b78c32cca0010/src/adtypes.jl#L1-L18">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../optimization_problem/">« Defining OptimizationProblems</a><a class="docs-footer-nextpage" href="../solve/">Common Solver Options (Solve Keyword Arguments) »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Tuesday 16 January 2024 08:37">Tuesday 16 January 2024</span>. Using Julia version 1.10.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
