<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Local Gradient-Based Optimization · GalacticOptim.jl</title><link rel="canonical" href="https://galacticoptim.sciml.ai/stable/local_optimizers/local_gradient/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="GalacticOptim.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">GalacticOptim.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">GalacticOptim.jl: Unified Global Optimization Package</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/intro/">Basic usage</a></li><li><a class="tocitem" href="../../tutorials/rosenbrock/">Rosenbrock function</a></li><li><a class="tocitem" href="../../tutorials/minibatch/">Minibatch</a></li></ul></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../../API/optimization_problem/">OptimizationProblem</a></li><li><a class="tocitem" href="../../API/optimization_function/">OptimizationFunction</a></li><li><a class="tocitem" href="../../API/solve/">solve</a></li><li><a class="tocitem" href="../../API/modelingtoolkit/">ModelingToolkit Integration</a></li></ul></li><li><span class="tocitem">Local Optimizers</span><ul><li class="is-active"><a class="tocitem" href>Local Gradient-Based Optimization</a><ul class="internal"><li><a class="tocitem" href="#Recommended-Methods"><span>Recommended Methods</span></a></li><li><a class="tocitem" href="#Flux.jl"><span>Flux.jl</span></a></li><li><a class="tocitem" href="#Optim.jl"><span>Optim.jl</span></a></li><li><a class="tocitem" href="#NLopt.jl"><span>NLopt.jl</span></a></li></ul></li><li><a class="tocitem" href="../local_derivative_free/">Local Derivative-Free Optimization</a></li><li><a class="tocitem" href="../local_hessian/">Local Hessian-Based Second Order Optimization</a></li><li><a class="tocitem" href="../local_hessian_free/">Local Hessian-Free Second Order Optimization</a></li></ul></li><li><span class="tocitem">Global Optimizers</span><ul><li><a class="tocitem" href="../../global_optimizers/global/">Global Unconstrained Optimizers</a></li><li><a class="tocitem" href="../../global_optimizers/global_constrained/">Global Constrained Optimization</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Local Optimizers</a></li><li class="is-active"><a href>Local Gradient-Based Optimization</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Local Gradient-Based Optimization</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/GalacticOptim.jl/blob/master/docs/src/local_optimizers/local_gradient.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Local-Gradient-Based-Optimization"><a class="docs-heading-anchor" href="#Local-Gradient-Based-Optimization">Local Gradient-Based Optimization</a><a id="Local-Gradient-Based-Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Local-Gradient-Based-Optimization" title="Permalink"></a></h1><h2 id="Recommended-Methods"><a class="docs-heading-anchor" href="#Recommended-Methods">Recommended Methods</a><a id="Recommended-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Recommended-Methods" title="Permalink"></a></h2><p><code>ADAM()</code> is a good default with decent convergence rate. <code>BFGS()</code> can converge faster but is more prone to hitting bad local optima. <code>LBFGS()</code> requires less memory than <code>BFGS</code> and thus can have better scaling.</p><h2 id="Flux.jl"><a class="docs-heading-anchor" href="#Flux.jl">Flux.jl</a><a id="Flux.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Flux.jl" title="Permalink"></a></h2><ul><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.Descent"><code>Flux.Optimise.Descent</code></a>: <strong>Classic gradient descent optimizer with learning rate</strong></p><ul><li><code>solve(problem, Descent(η))</code></li><li><code>η</code> is the learning rate</li><li>defaults to: <code>η = 0.1</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.Momentum"><code>Flux.Optimise.Momentum</code></a>: <strong>Classic gradient descent optimizer with learning rate and momentum</strong></p><ul><li><code>solve(problem, Momentum(η, ρ))</code></li><li><code>η</code> is the learning rate</li><li><code>ρ</code> is the momentum</li><li>defaults to: <code>η = 0.01, ρ = 0.9</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.Nesterov"><code>Flux.Optimise.Nesterov</code></a>: <strong>Gradient descent optimizer with learning rate and Nesterov momentum</strong></p><ul><li><code>solve(problem, Nesterov(η, ρ))</code></li><li><code>η</code> is the learning rate</li><li><code>ρ</code> is the Nesterov momentum</li><li>defaults to: <code>η = 0.01, ρ = 0.9</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.RMSProp"><code>Flux.Optimise.RMSProp</code></a>: <strong>RMSProp optimizer</strong></p><ul><li><code>solve(problem, RMSProp(η, ρ))</code></li><li><code>η</code> is the learning rate</li><li><code>ρ</code> is the momentum</li><li>defaults to: <code>η = 0.001, ρ = 0.9</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.ADAM"><code>Flux.Optimise.ADAM</code></a>: <strong>ADAM optimizer</strong></p><ul><li><code>solve(problem, ADAM(η, β::Tuple))</code></li><li><code>η</code> is the learning rate</li><li><code>β::Tuple</code> is the decay of momentums</li><li>defaults to: <code>η = 0.001, β::Tuple = (0.9, 0.999)</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.RADAM"><code>Flux.Optimise.RADAM</code></a>: <strong>Rectified ADAM optimizer</strong></p><ul><li><code>solve(problem, RADAM(η, β::Tuple))</code></li><li><code>η</code> is the learning rate</li><li><code>β::Tuple</code> is the decay of momentums</li><li>defaults to: <code>η = 0.001, β::Tuple = (0.9, 0.999)</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.AdaMax"><code>Flux.Optimise.AdaMax</code></a>: <strong>AdaMax optimizer</strong></p><ul><li><code>solve(problem, AdaMax(η, β::Tuple))</code></li><li><code>η</code> is the learning rate</li><li><code>β::Tuple</code> is the decay of momentums</li><li>defaults to: <code>η = 0.001, β::Tuple = (0.9, 0.999)</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.ADAGrad"><code>Flux.Optimise.ADAGRad</code></a>: <strong>ADAGrad optimizer</strong></p><ul><li><code>solve(problem, ADAGrad(η))</code></li><li><code>η</code> is the learning rate</li><li>defaults to: <code>η = 0.1</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.ADADelta"><code>Flux.Optimise.ADADelta</code></a>: <strong>ADADelta optimizer</strong></p><ul><li><code>solve(problem, ADADelta(ρ))</code></li><li><code>ρ</code> is the gradient decay factor</li><li>defaults to: <code>ρ = 0.9</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.ADAGrad"><code>Flux.Optimise.AMSGrad</code></a>: <strong>AMSGrad optimizer</strong></p><ul><li><code>solve(problem, AMSGrad(η, β::Tuple))</code></li><li><code>η</code> is the learning rate</li><li><code>β::Tuple</code> is the decay of momentums</li><li>defaults to: <code>η = 0.001, β::Tuple = (0.9, 0.999)</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.NADAM"><code>Flux.Optimise.NADAM</code></a>: <strong>Nesterov variant of the ADAM optimizer</strong></p><ul><li><code>solve(problem, NADAM(η, β::Tuple))</code></li><li><code>η</code> is the learning rate</li><li><code>β::Tuple</code> is the decay of momentums</li><li>defaults to: <code>η = 0.001, β::Tuple = (0.9, 0.999)</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.ADAMW"><code>Flux.Optimise.ADAMW</code></a>: <strong>ADAMW optimizer</strong></p><ul><li><code>solve(problem, ADAMW(η, β::Tuple))</code></li><li><code>η</code> is the learning rate</li><li><code>β::Tuple</code> is the decay of momentums</li><li><code>decay</code> is the decay to weights</li><li>defaults to: <code>η = 0.001, β::Tuple = (0.9, 0.999), decay = 0</code></li></ul></li></ul><h2 id="Optim.jl"><a class="docs-heading-anchor" href="#Optim.jl">Optim.jl</a><a id="Optim.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Optim.jl" title="Permalink"></a></h2><ul><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/cg/"><code>Optim.ConjugateGradient</code></a>: <strong>Conjugate Gradient Descent</strong></p><ul><li><code>solve(problem, ConjugateGradient(alphaguess, linesearch, eta, P, precondprep))</code></li><li><code>alphaguess</code> computes the initial step length (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_initialstep.html">this example</a>)<ul><li>available initial step length procedures:</li><li><code>InitialPrevious</code></li><li><code>InitialStatic</code></li><li><code>InitialHagerZhang</code></li><li><code>InitialQuadratic</code></li><li><code>InitialConstantChange</code></li></ul></li><li><code>linesearch</code> specifies the line search algorithm (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_linesearch.html">this example</a>)<ul><li>available line search algorithms:</li><li><code>HaegerZhang</code></li><li><code>MoreThuente</code></li><li><code>BackTracking</code></li><li><code>StrongWolfe</code></li><li><code>Static</code></li></ul></li><li><code>eta</code> determines the next step direction</li><li><code>P</code> is an optional preconditioner (for more information, see <a href="https://julianlsolvers.github.io/Optim.jl/v0.9.3/algo/precondition/">this source</a>)</li><li><code>precondpred</code> is used to update <code>P</code> as the state variable <code>x</code> changes</li><li>defaults to:</li></ul><p><code>julia   alphaguess = LineSearches.InitialHagerZhang(),   linesearch = LineSearches.HagerZhang(),   eta = 0.4,   P = nothing,   precondprep = (P, x) -&gt; nothing</code></p></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/gradientdescent/"><code>Optim.GradientDescent</code></a>: <strong>Gradient Descent (a quasi-Newton solver)</strong></p><ul><li><code>solve(problem, GradientDescent(alphaguess, linesearch, P, precondprep))</code></li><li><code>alphaguess</code> computes the initial step length (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_initialstep.html">this example</a>)<ul><li>available initial step length procedures:</li><li><code>InitialPrevious</code></li><li><code>InitialStatic</code></li><li><code>InitialHagerZhang</code></li><li><code>InitialQuadratic</code></li><li><code>InitialConstantChange</code></li></ul></li><li><code>linesearch</code> specifies the line search algorithm (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_linesearch.html">this example</a>)<ul><li>available line search algorithms:</li><li><code>HaegerZhang</code></li><li><code>MoreThuente</code></li><li><code>BackTracking</code></li><li><code>StrongWolfe</code></li><li><code>Static</code></li></ul></li><li><code>P</code> is an optional preconditioner (for more information, see <a href="https://julianlsolvers.github.io/Optim.jl/v0.9.3/algo/precondition/">this source</a>)</li><li><code>precondpred</code> is used to update <code>P</code> as the state variable <code>x</code> changes</li><li>defaults to:</li></ul><p><code>julia   alphaguess = LineSearches.InitialPrevious(),   linesearch = LineSearches.HagerZhang(),   P = nothing,   precondprep = (P, x) -&gt; nothing</code></p></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/lbfgs/"><code>Optim.BFGS</code></a>: <strong>Broyden-Fletcher-Goldfarb-Shanno algorithm</strong></p><ul><li><code>solve(problem, BFGS(alpaguess, linesearch, initial_invH, initial_stepnorm, manifold))</code></li><li><code>alphaguess</code> computes the initial step length (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_initialstep.html">this example</a>)<ul><li>available initial step length procedures:</li><li><code>InitialPrevious</code></li><li><code>InitialStatic</code></li><li><code>InitialHagerZhang</code></li><li><code>InitialQuadratic</code></li><li><code>InitialConstantChange</code></li></ul></li><li><code>linesearch</code> specifies the line search algorithm (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_linesearch.html">this example</a>)<ul><li>available line search algorithms:</li><li><code>HaegerZhang</code></li><li><code>MoreThuente</code></li><li><code>BackTracking</code></li><li><code>StrongWolfe</code></li><li><code>Static</code></li></ul></li><li><code>initial_invH</code> specifies an optional initial matrix</li><li><code>initial_stepnorm</code> determines that <code>initial_invH</code> is an identity matrix scaled by the value of <code>initial_stepnorm</code> multiplied by the sup-norm of the gradient at the initial point</li><li><code>manifold</code> specifies a (Riemannian) manifold on which the function is to be minimized (for more information, consult <a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/manifolds/">this source</a>)<ul><li>available manifolds:</li><li><code>Flat</code></li><li><code>Sphere</code></li><li><code>Stiefel</code></li><li>meta-manifolds:</li><li><code>PowerManifold</code></li><li><code>ProductManifold</code></li><li>custom manifolds</li></ul></li><li>defaults to: <code>alphaguess = LineSearches.InitialStatic()</code>, <code>linesearch = LineSearches.HagerZhang()</code>, <code>initial_invH = nothing</code>, <code>initial_stepnorm = nothing</code>, <code>manifold = Flat()</code></li></ul></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/lbfgs/"><code>Optim.LBFGS</code></a>: <strong>Limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm</strong></p></li></ul><h2 id="NLopt.jl"><a class="docs-heading-anchor" href="#NLopt.jl">NLopt.jl</a><a id="NLopt.jl-1"></a><a class="docs-heading-anchor-permalink" href="#NLopt.jl" title="Permalink"></a></h2></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../API/modelingtoolkit/">« ModelingToolkit Integration</a><a class="docs-footer-nextpage" href="../local_derivative_free/">Local Derivative-Free Optimization »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 6 February 2021 08:16">Saturday 6 February 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
