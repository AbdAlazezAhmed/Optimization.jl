<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Available optimizers · GalacticOptim.jl</title><link rel="canonical" href="https://galacticoptim.sciml.ai/stable/optimizers/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="GalacticOptim.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">GalacticOptim.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">GalacticOptim.jl: Unified Global Optimization Package</a></li><li><a class="tocitem" href="../usage/">Usage</a></li><li class="is-active"><a class="tocitem" href>Available optimizers</a><ul class="internal"><li><a class="tocitem" href="#Global-optimizers"><span>Global optimizers</span></a></li><li><a class="tocitem" href="#Local-gradient-based-optimizers"><span>Local gradient-based optimizers</span></a></li><li><a class="tocitem" href="#Local-derivative-free-optimizers"><span>Local derivative-free optimizers</span></a></li><li><a class="tocitem" href="#Second-order-optimizers"><span>Second-order optimizers</span></a></li><li><a class="tocitem" href="#Constrained-local-optimization"><span>Constrained local optimization</span></a></li><li><a class="tocitem" href="#Constrained-global-optimization"><span>Constrained global optimization</span></a></li></ul></li><li><a class="tocitem" href="../examples/">Examples</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Available optimizers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Available optimizers</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/GalacticOptim.jl/blob/master/docs/src/optimizers.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Available-optimizers-by-categories"><a class="docs-heading-anchor" href="#Available-optimizers-by-categories">Available optimizers by categories</a><a id="Available-optimizers-by-categories-1"></a><a class="docs-heading-anchor-permalink" href="#Available-optimizers-by-categories" title="Permalink"></a></h1><p>The first entries give the original name and package of the optimizers (and are linked to their source documentation). The comments in the next line describe the usage of the optimizers in GalacticOptim.jl and list the default values of the constructors.</p><p>It can be changed by doing: <code>solve(problem, optimizer, maxiters = 2000)</code>.</p><h2 id="Global-optimizers"><a class="docs-heading-anchor" href="#Global-optimizers">Global optimizers</a><a id="Global-optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Global-optimizers" title="Permalink"></a></h2><ul><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/particle_swarm/"><code>Optim.ParticleSwarm</code></a>: <strong>Particle Swarm Optimization</strong></p><ul><li><code>solve(problem, ParticleSwarm(lower, upper, n_particles))</code></li><li><code>lower</code>/<code>upper</code> are vectors of lower/upper bounds respectively</li><li><code>n_particles</code> is the number of particles in the swarm</li><li>defaults to: <code>lower = []</code>, <code>upper = []</code>, <code>n_particles = 0</code></li></ul></li><li><p><a href="https://github.com/robertfeldt/BlackBoxOptim.jl"><code>BlackBoxOptim</code></a>: <strong>(Meta-)heuristic/stochastic algorithms</strong></p><ul><li><code>solve(problem, BBO(method))</code></li><li>the name of the method must be preceded by <code>:</code>, for example: <code>:de_rand_2_bin</code></li><li>in GalacticOptim.jl, <code>BBO()</code> defaults to the recommended <code>adaptive_de_rand_1_bin_radiuslimited</code></li><li>the available methods are listed <a href="https://github.com/robertfeldt/BlackBoxOptim.jl#state-of-the-library">here</a></li></ul></li><li><p><a href="https://github.com/timholy/QuadDIRECT.jl"><code>QuadDIRECT</code></a>: <strong>QuadDIRECT algorithm (inspired by DIRECT and MCS)</strong></p><ul><li><code>solve(problem, QuadDirect(), splits)</code></li><li><code>splits</code> is a list of 3-vectors with initial locations at which to evaluate the function (the values must be in strictly increasing order and lie within the specified bounds), for instance:</li></ul><p><code>julia   prob = GalacticOptim.OptimizationProblem(f, x0, p, lb=[-3, -2], ub=[3, 2])   solve(prob, QuadDirect(), splits = ([-2, 0, 2], [-1, 0, 1]))</code></p><ul><li>also note that <code>QuadDIRECT</code> should (for now) be installed by doing: <code>] add https://github.com/timholy/QuadDIRECT.jl.git</code></li></ul></li><li><p><a href="https://wildart.github.io/Evolutionary.jl/stable/ga/"><code>Evolutionary.GA</code></a>: <strong>Genetic Algorithm optimizer</strong></p></li><li><p><a href="https://wildart.github.io/Evolutionary.jl/stable/es/"><code>Evolutionary.ES</code></a>: <strong>Evolution Strategy algorithm</strong></p></li><li><p><a href="https://wildart.github.io/Evolutionary.jl/stable/cmaes/"><code>Evolutionary.CMAES</code></a>: <strong>Covariance Matrix Adaptation Evolution Strategy algorithm</strong></p></li><li><p><a href="https://github.com/jbrea/CMAEvolutionStrategy.jl"><code>CMAEvolutionStrategy</code></a>: <strong>Covariance Matrix Adaptation Evolution Strategy algorithm</strong></p></li></ul><h2 id="Local-gradient-based-optimizers"><a class="docs-heading-anchor" href="#Local-gradient-based-optimizers">Local gradient-based optimizers</a><a id="Local-gradient-based-optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Local-gradient-based-optimizers" title="Permalink"></a></h2><ul><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.Descent"><code>Flux.Optimise.Descent</code></a>: <strong>Classic gradient descent optimizer with learning rate</strong></p><ul><li><code>solve(problem, Descent(η))</code></li><li><code>η</code> is the learning rate</li><li>defaults to: <code>η = 0.1</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.Momentum"><code>Flux.Optimise.Momentum</code></a>: <strong>Classic gradient descent optimizer with learning rate and momentum</strong></p><ul><li><code>solve(problem, Momentum(η, ρ))</code></li><li><code>η</code> is the learning rate</li><li><code>ρ</code> is the momentum</li><li>defaults to: <code>η = 0.01, ρ = 0.9</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.Nesterov"><code>Flux.Optimise.Nesterov</code></a>: <strong>Gradient descent optimizer with learning rate and Nesterov momentum</strong></p><ul><li><code>solve(problem, Nesterov(η, ρ))</code></li><li><code>η</code> is the learning rate</li><li><code>ρ</code> is the Nesterov momentum</li><li>defaults to: <code>η = 0.01, ρ = 0.9</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.RMSProp"><code>Flux.Optimise.RMSProp</code></a>: <strong>RMSProp optimizer</strong></p><ul><li><code>solve(problem, RMSProp(η, ρ))</code></li><li><code>η</code> is the learning rate</li><li><code>ρ</code> is the momentum</li><li>defaults to: <code>η = 0.001, ρ = 0.9</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.ADAM"><code>Flux.Optimise.ADAM</code></a>: <strong>ADAM optimizer</strong></p><ul><li><code>solve(problem, ADAM(η, β::Tuple))</code></li><li><code>η</code> is the learning rate</li><li><code>β::Tuple</code> is the decay of momentums</li><li>defaults to: <code>η = 0.001, β::Tuple = (0.9, 0.999)</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.RADAM"><code>Flux.Optimise.RADAM</code></a>: <strong>Rectified ADAM optimizer</strong></p><ul><li><code>solve(problem, RADAM(η, β::Tuple))</code></li><li><code>η</code> is the learning rate</li><li><code>β::Tuple</code> is the decay of momentums</li><li>defaults to: <code>η = 0.001, β::Tuple = (0.9, 0.999)</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.AdaMax"><code>Flux.Optimise.AdaMax</code></a>: <strong>AdaMax optimizer</strong></p><ul><li><code>solve(problem, AdaMax(η, β::Tuple))</code></li><li><code>η</code> is the learning rate</li><li><code>β::Tuple</code> is the decay of momentums</li><li>defaults to: <code>η = 0.001, β::Tuple = (0.9, 0.999)</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.ADAGrad"><code>Flux.Optimise.ADAGRad</code></a>: <strong>ADAGrad optimizer</strong></p><ul><li><code>solve(problem, ADAGrad(η))</code></li><li><code>η</code> is the learning rate</li><li>defaults to: <code>η = 0.1</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.ADADelta"><code>Flux.Optimise.ADADelta</code></a>: <strong>ADADelta optimizer</strong></p><ul><li><code>solve(problem, ADADelta(ρ))</code></li><li><code>ρ</code> is the gradient decay factor</li><li>defaults to: <code>ρ = 0.9</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.ADAGrad"><code>Flux.Optimise.AMSGrad</code></a>: <strong>AMSGrad optimizer</strong></p><ul><li><code>solve(problem, AMSGrad(η, β::Tuple))</code></li><li><code>η</code> is the learning rate</li><li><code>β::Tuple</code> is the decay of momentums</li><li>defaults to: <code>η = 0.001, β::Tuple = (0.9, 0.999)</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.NADAM"><code>Flux.Optimise.NADAM</code></a>: <strong>Nesterov variant of the ADAM optimizer</strong></p><ul><li><code>solve(problem, NADAM(η, β::Tuple))</code></li><li><code>η</code> is the learning rate</li><li><code>β::Tuple</code> is the decay of momentums</li><li>defaults to: <code>η = 0.001, β::Tuple = (0.9, 0.999)</code></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.ADAMW"><code>Flux.Optimise.ADAMW</code></a>: <strong>ADAMW optimizer</strong></p><ul><li><code>solve(problem, ADAMW(η, β::Tuple))</code></li><li><code>η</code> is the learning rate</li><li><code>β::Tuple</code> is the decay of momentums</li><li><code>decay</code> is the decay to weights</li><li>defaults to: <code>η = 0.001, β::Tuple = (0.9, 0.999), decay = 0</code></li></ul></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/cg/"><code>Optim.ConjugateGradient</code></a>: <strong>Conjugate Gradient Descent</strong></p><ul><li><code>solve(problem, ConjugateGradient(alphaguess, linesearch, eta, P, precondprep))</code></li><li><code>alphaguess</code> computes the initial step length (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_initialstep.html">this example</a>)<ul><li>available initial step length procedures:</li><li><code>InitialPrevious</code></li><li><code>InitialStatic</code></li><li><code>InitialHagerZhang</code></li><li><code>InitialQuadratic</code></li><li><code>InitialConstantChange</code></li></ul></li><li><code>linesearch</code> specifies the line search algorithm (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_linesearch.html">this example</a>)<ul><li>available line search algorithms:</li><li><code>HaegerZhang</code></li><li><code>MoreThuente</code></li><li><code>BackTracking</code></li><li><code>StrongWolfe</code></li><li><code>Static</code></li></ul></li><li><code>eta</code> determines the next step direction</li><li><code>P</code> is an optional preconditioner (for more information, see <a href="https://julianlsolvers.github.io/Optim.jl/v0.9.3/algo/precondition/">this source</a>)</li><li><code>precondpred</code> is used to update <code>P</code> as the state variable <code>x</code> changes</li><li>defaults to:</li></ul><p><code>julia   alphaguess = LineSearches.InitialHagerZhang(),   linesearch = LineSearches.HagerZhang(),   eta = 0.4,   P = nothing,   precondprep = (P, x) -&gt; nothing</code></p></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/gradientdescent/"><code>Optim.GradientDescent</code></a>: <strong>Gradient Descent (a quasi-Newton solver)</strong></p><ul><li><code>solve(problem, GradientDescent(alphaguess, linesearch, P, precondprep))</code></li><li><code>alphaguess</code> computes the initial step length (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_initialstep.html">this example</a>)<ul><li>available initial step length procedures:</li><li><code>InitialPrevious</code></li><li><code>InitialStatic</code></li><li><code>InitialHagerZhang</code></li><li><code>InitialQuadratic</code></li><li><code>InitialConstantChange</code></li></ul></li><li><code>linesearch</code> specifies the line search algorithm (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_linesearch.html">this example</a>)<ul><li>available line search algorithms:</li><li><code>HaegerZhang</code></li><li><code>MoreThuente</code></li><li><code>BackTracking</code></li><li><code>StrongWolfe</code></li><li><code>Static</code></li></ul></li><li><code>P</code> is an optional preconditioner (for more information, see <a href="https://julianlsolvers.github.io/Optim.jl/v0.9.3/algo/precondition/">this source</a>)</li><li><code>precondpred</code> is used to update <code>P</code> as the state variable <code>x</code> changes</li><li>defaults to:</li></ul><p><code>julia   alphaguess = LineSearches.InitialPrevious(),   linesearch = LineSearches.HagerZhang(),   P = nothing,   precondprep = (P, x) -&gt; nothing</code></p></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/lbfgs/"><code>Optim.BFGS</code></a>: <strong>Broyden-Fletcher-Goldfarb-Shanno algorithm</strong></p><ul><li><code>solve(problem, BFGS(alpaguess, linesearch, initial_invH, initial_stepnorm, manifold))</code></li><li><code>alphaguess</code> computes the initial step length (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_initialstep.html">this example</a>)<ul><li>available initial step length procedures:</li><li><code>InitialPrevious</code></li><li><code>InitialStatic</code></li><li><code>InitialHagerZhang</code></li><li><code>InitialQuadratic</code></li><li><code>InitialConstantChange</code></li></ul></li><li><code>linesearch</code> specifies the line search algorithm (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_linesearch.html">this example</a>)<ul><li>available line search algorithms:</li><li><code>HaegerZhang</code></li><li><code>MoreThuente</code></li><li><code>BackTracking</code></li><li><code>StrongWolfe</code></li><li><code>Static</code></li></ul></li><li><code>initial_invH</code> specifies an optional initial matrix</li><li><code>initial_stepnorm</code> determines that <code>initial_invH</code> is an identity matrix scaled by the value of <code>initial_stepnorm</code> multiplied by the sup-norm of the gradient at the initial point</li><li><code>manifold</code> specifies a (Riemannian) manifold on which the function is to be minimized (for more information, consult <a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/manifolds/">this source</a>)<ul><li>available manifolds:</li><li><code>Flat</code></li><li><code>Sphere</code></li><li><code>Stiefel</code></li><li>meta-manifolds:</li><li><code>PowerManifold</code></li><li><code>ProductManifold</code></li><li>custom manifolds</li></ul></li><li>defaults to: <code>alphaguess = LineSearches.InitialStatic()</code>, <code>linesearch = LineSearches.HagerZhang()</code>, <code>initial_invH = nothing</code>, <code>initial_stepnorm = nothing</code>, <code>manifold = Flat()</code></li></ul></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/lbfgs/"><code>Optim.LBFGS</code></a>: <strong>Limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm</strong></p></li></ul><h2 id="Local-derivative-free-optimizers"><a class="docs-heading-anchor" href="#Local-derivative-free-optimizers">Local derivative-free optimizers</a><a id="Local-derivative-free-optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Local-derivative-free-optimizers" title="Permalink"></a></h2><ul><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/nelder_mead/"><code>Optim.NelderMead</code></a>: <strong>Nelder-Mead optimizer</strong></p><ul><li><code>solve(problem, NelderMead(parameters, initial_simplex))</code></li><li><code>parameters = AdaptiveParameters()</code> or <code>parameters = FixedParameters()</code></li><li><code>initial_simplex = AffineSimplexer()</code></li><li>defaults to: <code>parameters = AdaptiveParameters(), initial_simplex = AffineSimplexer()</code></li></ul></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/simulated_annealing/"><code>Optim.SimulatedAnnealing</code></a>: <strong>Simulated Annealing</strong></p><ul><li><code>solve(problem, SimulatedAnnealing(neighbor, T, p))</code></li><li><code>neighbor</code> is a mutating function of the current and proposed <code>x</code></li><li><code>T</code> is a function of the current iteration that returns a temperature</li><li><code>p</code> is a function of the current temperature</li><li>defaults to: <code>neighbor = default_neighbor!, T = default_temperature, p = kirkpatrick</code></li></ul></li></ul><h2 id="Second-order-optimizers"><a class="docs-heading-anchor" href="#Second-order-optimizers">Second-order optimizers</a><a id="Second-order-optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Second-order-optimizers" title="Permalink"></a></h2><ul><li><a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/newton/"><code>Optim.Newton</code></a></li></ul><h2 id="Constrained-local-optimization"><a class="docs-heading-anchor" href="#Constrained-local-optimization">Constrained local optimization</a><a id="Constrained-local-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Constrained-local-optimization" title="Permalink"></a></h2><ul><li><a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/ipnewton/"><code>Optim.IPNewton</code></a></li></ul><h2 id="Constrained-global-optimization"><a class="docs-heading-anchor" href="#Constrained-global-optimization">Constrained global optimization</a><a id="Constrained-global-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Constrained-global-optimization" title="Permalink"></a></h2><ul><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/samin/"><code>Optim.SAMIN</code></a>: <strong>Simulated Annealing with bounds</strong></p><ul><li><code>solve(problem, SAMIN(nt, ns, rt, neps, f_tol, x_tol, coverage_ok, verbosity))</code></li><li>defaults to:</li></ul><p>```julia   SAMIN(; nt::Int = 5  # reduce temperature every nt<em>ns</em>dim(x<em>init) evaluations           ns::Int = 5  # adjust bounds every ns*dim(x</em>init) evaluations           rt::T = 0.9  # geometric temperature reduction factor: when temp changes, new temp is t=rt*t           neps::Int = 5  # number of previous best values the final result is compared to           f<em>tol::T = 1e-12  # the required tolerance level for function value comparisons           x</em>tol::T = 1e-6  # the required tolerance level for x           coverage_ok::Bool = false,  # if false, increase temperature until initial parameter space is covered           verbosity::Int = 0)  # scalar: 0, 1, 2 or 3 (default = 0).</p><h1>copied verbatim from https://julianlsolvers.github.io/Optim.jl/stable/#algo/samin/#constructor</h1><p>```</p></li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../usage/">« Usage</a><a class="docs-footer-nextpage" href="../examples/">Examples »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 22 December 2020 16:36">Tuesday 22 December 2020</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
