<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Basic usage · Optimization.jl</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-90474609-3"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-90474609-3', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://Optimization.sciml.ai/stable/tutorials/intro/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Optimization.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Optimization.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Optimization.jl: A Unified Optimization Package</a></li><li><span class="tocitem">Tutorials</span><ul><li class="is-active"><a class="tocitem" href>Basic usage</a><ul class="internal"><li><a class="tocitem" href="#Controlling-Gradient-Calculations-(Automatic-Differentiation)"><span>Controlling Gradient Calculations (Automatic Differentiation)</span></a></li><li><a class="tocitem" href="#Setting-Box-Constraints"><span>Setting Box Constraints</span></a></li></ul></li><li><a class="tocitem" href="../rosenbrock/">Solving the Rosenbrock Problem in &gt;10 Ways</a></li><li><a class="tocitem" href="../minibatch/">Data Iterators and Minibatching</a></li><li><a class="tocitem" href="../symbolic/">Symbolic Problem Building with ModelingToolkit</a></li></ul></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../../API/optimization_problem/">Defining OptimizationProblems</a></li><li><a class="tocitem" href="../../API/optimization_function/">OptimizationFunction</a></li><li><a class="tocitem" href="../../API/solve/">Common Solver Options (Solve Keyword Arguments)</a></li><li><a class="tocitem" href="../../API/modelingtoolkit/">ModelingToolkit Integration</a></li></ul></li><li><span class="tocitem">Optimizer Packages</span><ul><li><a class="tocitem" href="../../optimization_packages/blackboxoptim/">BlackBoxOptim.jl</a></li><li><a class="tocitem" href="../../optimization_packages/cmaevolutionstrategy/">CMAEvolutionStrategy.jl</a></li><li><a class="tocitem" href="../../optimization_packages/evolutionary/">Evolutionary.jl</a></li><li><a class="tocitem" href="../../optimization_packages/flux/">Flux.jl</a></li><li><a class="tocitem" href="../../optimization_packages/gcmaes/">GCMAES.jl</a></li><li><a class="tocitem" href="../../optimization_packages/mathoptinterface/">MathOptInterface.jl</a></li><li><a class="tocitem" href="../../optimization_packages/multistartoptimization/">MultistartOptimization.jl</a></li><li><a class="tocitem" href="../../optimization_packages/metaheuristics/">Metaheuristics.jl</a></li><li><a class="tocitem" href="../../optimization_packages/nomad/">NOMAD.jl</a></li><li><a class="tocitem" href="../../optimization_packages/nlopt/">NLopt.jl</a></li><li><a class="tocitem" href="../../optimization_packages/nonconvex/">Nonconvex.jl</a></li><li><a class="tocitem" href="../../optimization_packages/optim/">Optim.jl</a></li><li><a class="tocitem" href="../../optimization_packages/quaddirect/">QuadDIRECT.jl</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Basic usage</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Basic usage</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/Optimization.jl/blob/master/docs/src/tutorials/intro.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Basic-usage"><a class="docs-heading-anchor" href="#Basic-usage">Basic usage</a><a id="Basic-usage-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-usage" title="Permalink"></a></h1><p>In this tutorial we introduce the basics of Optimization.jl by showing how to easily mix local optimizers from Optim.jl and global optimizers from BlackBoxOptim.jl on the Rosenbrock equation. The simplest copy-pasteable code to get started is the following:</p><pre><code class="language-julia hljs"># Import the package and define the problem to optimize
using Optimization
rosenbrock(u,p) =  (u[1] - u[1])^2 + p[2] * (u[2] - u[1]^2)^2
u0 = zeros(2)
p  = [1.0,100.0]

prob = OptimizationProblem(rosenbrock,u0,p)

# Import a solver package and solve the optimization problem
using OptimizationOptimJL
sol = solve(prob,NelderMead())

# Import a different solver package and solve the optimization problem a different way
using OptimizationBBO
prob = OptimizationProblem(rosenbrock, u0, p, lb = [-1.0,-1.0], ub = [1.0,1.0])
sol = solve(prob,BBO_adaptive_de_rand_1_bin_radiuslimited())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">u: 2-element Vector{Float64}:
 -0.32494014351428124
  0.10558609689558635</code></pre><p>Notice that Optimization.jl is the core glue package that holds all of the common pieces, but to solve the equations we need to use a solver package. Here, OptimizationOptimJL is for <a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a> and OptimizationBBO is for <a href="https://github.com/robertfeldt/BlackBoxOptim.jl">BlackBoxOptim.jl</a>.</p><p>The output of the first optimization task (with the <code>NelderMead()</code> algorithm) is given below:</p><pre><code class="language- hljs">sol = solve(prob,NelderMead())</code></pre><p>The solution from the original solver can always be obtained via <code>original</code>:</p><pre><code class="language-julia hljs">sol.original</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">BlackBoxOptim.OptimizationResults(&quot;adaptive_de_rand_1_bin_radiuslimited&quot;, &quot;Max number of steps (10000) reached&quot;, 10001, 1.655686151412975e9, 0.2345268726348877, BlackBoxOptim.ParamsDictChain[BlackBoxOptim.ParamsDictChain[Dict{Symbol, Any}(:CallbackInterval =&gt; 0.0, :RngSeed =&gt; 293113, :SearchRange =&gt; [(-1.0, 1.0), (-1.0, 1.0)], :Method =&gt; :adaptive_de_rand_1_bin_radiuslimited, :CallbackFunction =&gt; OptimizationBBO.var&quot;#_cb#23&quot;{OptimizationBBO.var&quot;#21#25&quot;, Base.Iterators.Cycle{Tuple{Optimization.NullData}}}(OptimizationBBO.var&quot;#21#25&quot;(), Base.Iterators.Cycle{Tuple{Optimization.NullData}}((Optimization.NullData(),)), Core.Box(1.9827679158928683e-12), Core.Box(Optimization.NullData()), Core.Box(2)), :MaxSteps =&gt; 10000),Dict{Symbol, Any}()],Dict{Symbol, Any}(:CallbackInterval =&gt; -1.0, :TargetFitness =&gt; nothing, :TraceMode =&gt; :compact, :FitnessScheme =&gt; BlackBoxOptim.ScalarFitnessScheme{true}(), :MinDeltaFitnessTolerance =&gt; 1.0e-50, :NumDimensions =&gt; :NotSpecified, :FitnessTolerance =&gt; 1.0e-8, :TraceInterval =&gt; 0.5, :MaxStepsWithoutProgress =&gt; 10000, :MaxSteps =&gt; 10000…)], 10215, BlackBoxOptim.ScalarFitnessScheme{true}(), BlackBoxOptim.TopListArchiveOutput{Float64, Vector{Float64}}(8.125154688708371e-20, [-0.32494014351428124, 0.10558609689558635]), BlackBoxOptim.PopulationOptimizerOutput{BlackBoxOptim.FitPopulation{Float64}}(BlackBoxOptim.FitPopulation{Float64}([-0.3221565979907395 -0.32208865174757356 … -0.3225284511508626 -0.3221442554328433; 0.10378487125561212 0.10374111005923287 … 0.10402461523874926 0.10377693601039546], NaN, [5.632813093217033e-16, 1.0971856812145122e-14, 1.0971856812145122e-14, 1.3033078901420818e-15, 2.7155812956715455e-15, 5.632813093217033e-16, 2.938880347991625e-14, 6.462295540531436e-19, 7.234851475734446e-16, 2.7363099156992292e-16  …  1.8387974864058476e-12, 3.897653070110933e-13, 5.196434769864751e-14, 5.948666767560758e-15, 1.1175193803992006e-14, 2.179492017008256e-14, 3.928741588201873e-14, 1.893691094548316e-14, 1.805522942983755e-14, 2.161492293867227e-14], 0, BlackBoxOptim.Candidate{Float64}[BlackBoxOptim.Candidate{Float64}([-0.324947496632417, 0.10559090397836823], 22, 8.071675094414724e-14, BlackBoxOptim.AdaptiveDiffEvoRandBin{3}(BlackBoxOptim.AdaptiveDiffEvoParameters(BlackBoxOptim.BimodalCauchy(Distributions.Cauchy{Float64}(μ=0.65, σ=0.1), Distributions.Cauchy{Float64}(μ=1.0, σ=0.1), 0.5, false, true), BlackBoxOptim.BimodalCauchy(Distributions.Cauchy{Float64}(μ=0.1, σ=0.1), Distributions.Cauchy{Float64}(μ=0.95, σ=0.1), 0.5, false, true), [0.8854118587682991, 1.0, 0.8603984115441884, 0.771023637629296, 1.0, 0.6884777807964333, 0.9566886050530794, 0.44923532764708063, 1.0, 0.5425642216010924  …  0.6636913968912549, 0.49853227686667695, 0.7065485837495792, 0.8783906856972307, 0.8564862032873752, 1.0, 0.9941971739738844, 1.0, 0.9929737900029283, 0.9065395409019169], [0.9466577224680086, 0.8323867084558819, 0.15317413662298768, 0.9272783116414357, 0.7877297599379196, 0.9192538595310022, 0.9499513335774156, 0.10337363709638023, 0.35860877047268913, 1.0  …  0.9549074943278468, 1.0, 0.14008151081899511, 0.08343674501991073, 0.019664264036010737, 0.7206829870868936, 0.12477511546514183, 0.0002691293336799655, 1.0, 0.9499932768618095])), 0), BlackBoxOptim.Candidate{Float64}([-0.3251884780914301, 0.10574768709421256], 22, 1.9827679158928683e-12, BlackBoxOptim.AdaptiveDiffEvoRandBin{3}(BlackBoxOptim.AdaptiveDiffEvoParameters(BlackBoxOptim.BimodalCauchy(Distributions.Cauchy{Float64}(μ=0.65, σ=0.1), Distributions.Cauchy{Float64}(μ=1.0, σ=0.1), 0.5, false, true), BlackBoxOptim.BimodalCauchy(Distributions.Cauchy{Float64}(μ=0.1, σ=0.1), Distributions.Cauchy{Float64}(μ=0.95, σ=0.1), 0.5, false, true), [0.8854118587682991, 1.0, 0.8603984115441884, 0.771023637629296, 1.0, 0.6884777807964333, 0.9566886050530794, 0.44923532764708063, 1.0, 0.5425642216010924  …  0.6636913968912549, 0.49853227686667695, 0.7065485837495792, 0.8783906856972307, 0.8564862032873752, 1.0, 0.9941971739738844, 1.0, 0.9929737900029283, 0.9065395409019169], [0.9466577224680086, 0.8323867084558819, 0.15317413662298768, 0.9272783116414357, 0.7877297599379196, 0.9192538595310022, 0.9499513335774156, 0.10337363709638023, 0.35860877047268913, 1.0  …  0.9549074943278468, 1.0, 0.14008151081899511, 0.08343674501991073, 0.019664264036010737, 0.7206829870868936, 0.12477511546514183, 0.0002691293336799655, 1.0, 0.9499932768618095])), 0)], Base.Threads.SpinLock(0))))</code></pre><h2 id="Controlling-Gradient-Calculations-(Automatic-Differentiation)"><a class="docs-heading-anchor" href="#Controlling-Gradient-Calculations-(Automatic-Differentiation)">Controlling Gradient Calculations (Automatic Differentiation)</a><a id="Controlling-Gradient-Calculations-(Automatic-Differentiation)-1"></a><a class="docs-heading-anchor-permalink" href="#Controlling-Gradient-Calculations-(Automatic-Differentiation)" title="Permalink"></a></h2><p>Notice that both of the above methods were derivative-free methods, and thus no gradients were required to do the optimization. However, in many cases first order optimization (i.e. using gradients) is much more efficient. Defining gradients can be done in two ways. One way is to manually provide a gradient definition in the <code>OptimizationFunction</code> constructor. However, the more convenient way to obtain gradients is to provide an AD backend type. </p><p>For example, let&#39;s now use the OptimizationOptimJL <code>BFGS</code> method to solve the same problem. We will import the forward-mode automatic differentiation library (<code>using ForwardDiff</code>) and then specify in the <code>OptimizationFunction</code> to automatically construct the derivative functions using ForwardDiff.jl. This looks like:</p><pre><code class="language-julia hljs">using ForwardDiff
optf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())
prob = OptimizationProblem(optf, u0, p)
sol = solve(prob,BFGS())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">u: 2-element Vector{Float64}:
 0.0
 0.0</code></pre><p>We can inspect the <code>original</code> to see the statistics on the number of steps  required and gradients computed:</p><pre><code class="language-julia hljs">sol.original</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi"> * Status: success

 * Candidate solution
    Final objective value:     0.000000e+00

 * Found with
    Algorithm:     BFGS

 * Convergence measures
    |x - x&#39;|               = 0.00e+00 ≤ 0.0e+00
    |x - x&#39;|/|x&#39;|          = NaN ≰ 0.0e+00
    |f(x) - f(x&#39;)|         = NaN ≰ 0.0e+00
    |f(x) - f(x&#39;)|/|f(x&#39;)| = NaN ≰ 0.0e+00
    |g(x)|                 = 0.00e+00 ≤ 1.0e-08

 * Work counters
    Seconds run:   0  (vs limit Inf)
    Iterations:    0
    f(x) calls:    1
    ∇f(x) calls:   1
</code></pre><p>Sure enough, it&#39;s a lot less than the derivative-free methods!</p><p>However, the compute cost of forward-mode automatic differentiation scales via the number of inputs, and thus as our optimization problem grows large it slow down. To counteract this, for larger optimization problems (&gt;100 state variables) one normally would want to use reverse-mode automatic differentiation. One common choice for reverse-mode automatic differentiation is Zygote.jl. We can demonstrate this via:</p><pre><code class="language-julia hljs">using Zygote
optf = OptimizationFunction(rosenbrock, Optimization.AutoZygote())
prob = OptimizationProblem(optf, u0, p)
sol = solve(prob,BFGS())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">u: 2-element Vector{Float64}:
 0.0
 0.0</code></pre><h2 id="Setting-Box-Constraints"><a class="docs-heading-anchor" href="#Setting-Box-Constraints">Setting Box Constraints</a><a id="Setting-Box-Constraints-1"></a><a class="docs-heading-anchor-permalink" href="#Setting-Box-Constraints" title="Permalink"></a></h2><p>In many cases one knows the potential bounds on the solution values. In Optimization.jl, these can be supplied as the <code>lb</code> and <code>ub</code> arguments for the lower bounds and upper bounds respectively, supplying a vector of values with one per state variable. Let&#39;s now do our gradient-based optimization with box constraints by rebuilding the OptimizationProblem:</p><pre><code class="language-julia hljs">prob = OptimizationProblem(optf, u0, p, lb = [-1.0,-1.0], ub = [1.0,1.0])
sol = solve(prob,BFGS())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">u: 2-element Vector{Float64}:
 0.0
 0.0</code></pre><p>For more information on handling constraints, in particular equality and inequality constraints, take a look at the <a href="../constraints/#constraints">constraints tutorial</a>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« Optimization.jl: A Unified Optimization Package</a><a class="docs-footer-nextpage" href="../rosenbrock/">Solving the Rosenbrock Problem in &gt;10 Ways »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.19 on <span class="colophon-date" title="Monday 20 June 2022 00:51">Monday 20 June 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
