<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Local Gradient-Based Optimization · GalacticOptim.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://galacticoptim.sciml.ai/stable/local_optimizers/local_gradient/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="GalacticOptim.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">GalacticOptim.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">GalacticOptim.jl: Unified Global Optimization Package</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/intro/">Basic usage</a></li><li><a class="tocitem" href="../../tutorials/rosenbrock/">Rosenbrock function</a></li><li><a class="tocitem" href="../../tutorials/minibatch/">Minibatch</a></li><li><a class="tocitem" href="../../tutorials/symbolic/">Symbolic Modeling</a></li></ul></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../../API/optimization_problem/">OptimizationProblem</a></li><li><a class="tocitem" href="../../API/optimization_function/">OptimizationFunction</a></li><li><a class="tocitem" href="../../API/solve/">solve</a></li><li><a class="tocitem" href="../../API/modelingtoolkit/">ModelingToolkit Integration</a></li></ul></li><li><span class="tocitem">Optimizer Packages</span><ul><li><a class="tocitem" href="../../optimization_packages/blackboxoptim/">BlackBoxOptim.jl</a></li><li><a class="tocitem" href="../../optimization_packages/cmaevolutionstrategy/">CMAEvolutionStrategy.jl</a></li><li><a class="tocitem" href="../../optimization_packages/evolutionary/">Evolutionary.jl</a></li><li><a class="tocitem" href="../../optimization_packages/flux/">Flux.jl</a></li><li><a class="tocitem" href="../../optimization_packages/mathoptinterface/">MathOptInterface.jl</a></li><li><a class="tocitem" href="../../optimization_packages/multistartoptimization/">MultistartOptimization.jl</a></li><li><a class="tocitem" href="../../optimization_packages/metaheuristics/">Metaheuristics.jl</a></li><li><a class="tocitem" href="../../optimization_packages/nlopt/">NLopt.jl</a></li><li><a class="tocitem" href="../../optimization_packages/nonconvex/">Nonconvex.jl</a></li><li><a class="tocitem" href="../../optimization_packages/optim/">Optim.jl</a></li><li><a class="tocitem" href="../../optimization_packages/quaddirect/">QuadDIRECT.jl</a></li></ul></li><li><span class="tocitem">Local Optimizers</span><ul><li class="is-active"><a class="tocitem" href>Local Gradient-Based Optimization</a><ul class="internal"><li><a class="tocitem" href="#Recommended-Methods"><span>Recommended Methods</span></a></li><li><a class="tocitem" href="#Flux.jl"><span>Flux.jl</span></a></li><li><a class="tocitem" href="#Optim.jl"><span>Optim.jl</span></a></li><li><a class="tocitem" href="#Ipopt.jl-(MathOptInterface)"><span>Ipopt.jl (MathOptInterface)</span></a></li><li><a class="tocitem" href="#NLopt.jl"><span>NLopt.jl</span></a></li></ul></li><li><a class="tocitem" href="../local_derivative_free/">Local Derivative-Free Optimization</a></li><li><a class="tocitem" href="../local_hessian/">Local Hessian-Based Second Order Optimization</a></li><li><a class="tocitem" href="../local_hessian_free/">Local Hessian-Free Second Order Optimization</a></li></ul></li><li><span class="tocitem">Global Optimizers</span><ul><li><a class="tocitem" href="../../global_optimizers/global/">Global Unconstrained Optimizers</a></li><li><a class="tocitem" href="../../global_optimizers/global_constrained/">Global Constrained Optimization</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Local Optimizers</a></li><li class="is-active"><a href>Local Gradient-Based Optimization</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Local Gradient-Based Optimization</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/GalacticOptim.jl/blob/master/docs/src/local_optimizers/local_gradient.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Local-Gradient-Based-Optimization"><a class="docs-heading-anchor" href="#Local-Gradient-Based-Optimization">Local Gradient-Based Optimization</a><a id="Local-Gradient-Based-Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Local-Gradient-Based-Optimization" title="Permalink"></a></h1><h2 id="Recommended-Methods"><a class="docs-heading-anchor" href="#Recommended-Methods">Recommended Methods</a><a id="Recommended-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Recommended-Methods" title="Permalink"></a></h2><p><code>ADAM()</code> is a good default with decent convergence rate. <code>BFGS()</code> can converge faster but is more prone to hitting bad local optima. <code>LBFGS()</code> requires less memory than <code>BFGS</code> and thus can have better scaling.</p><h2 id="Flux.jl"><a class="docs-heading-anchor" href="#Flux.jl">Flux.jl</a><a id="Flux.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Flux.jl" title="Permalink"></a></h2><ul><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.Descent"><code>Flux.Optimise.Descent</code></a>: <strong>Classic gradient descent optimizer with learning rate</strong></p><ul><li><code>solve(problem, Descent(η))</code></li><li><code>η</code> is the learning rate</li><li>Defaults:<ul><li><code>η = 0.1</code></li></ul></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.Momentum"><code>Flux.Optimise.Momentum</code></a>: <strong>Classic gradient descent optimizer with learning rate and momentum</strong></p><ul><li><code>solve(problem, Momentum(η, ρ))</code></li><li><code>η</code> is the learning rate</li><li><code>ρ</code> is the momentum</li><li>Defaults:<ul><li><code>η = 0.01</code></li><li><code>ρ = 0.9</code></li></ul></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.Nesterov"><code>Flux.Optimise.Nesterov</code></a>: <strong>Gradient descent optimizer with learning rate and Nesterov momentum</strong></p><ul><li><code>solve(problem, Nesterov(η, ρ))</code></li><li><code>η</code> is the learning rate</li><li><code>ρ</code> is the Nesterov momentum</li><li>Defaults:<ul><li><code>η = 0.01</code></li><li><code>ρ = 0.9</code></li></ul></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.RMSProp"><code>Flux.Optimise.RMSProp</code></a>: <strong>RMSProp optimizer</strong></p><ul><li><code>solve(problem, RMSProp(η, ρ))</code></li><li><code>η</code> is the learning rate</li><li><code>ρ</code> is the momentum</li><li>Defaults:<ul><li><code>η = 0.001</code></li><li><code>ρ = 0.9</code></li></ul></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.ADAM"><code>Flux.Optimise.ADAM</code></a>: <strong>ADAM optimizer</strong></p><ul><li><code>solve(problem, ADAM(η, β::Tuple))</code></li><li><code>η</code> is the learning rate</li><li><code>β::Tuple</code> is the decay of momentums</li><li>Defaults:<ul><li><code>η = 0.001</code></li><li><code>β::Tuple = (0.9, 0.999)</code></li></ul></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.RADAM"><code>Flux.Optimise.RADAM</code></a>: <strong>Rectified ADAM optimizer</strong></p><ul><li><code>solve(problem, RADAM(η, β::Tuple))</code></li><li><code>η</code> is the learning rate</li><li><code>β::Tuple</code> is the decay of momentums</li><li>Defaults:<ul><li><code>η = 0.001</code></li><li><code>β::Tuple = (0.9, 0.999)</code></li></ul></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.AdaMax"><code>Flux.Optimise.AdaMax</code></a>: <strong>AdaMax optimizer</strong></p><ul><li><code>solve(problem, AdaMax(η, β::Tuple))</code></li><li><code>η</code> is the learning rate</li><li><code>β::Tuple</code> is the decay of momentums</li><li>Defaults:<ul><li><code>η = 0.001</code></li><li><code>β::Tuple = (0.9, 0.999)</code></li></ul></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.ADAGrad"><code>Flux.Optimise.ADAGRad</code></a>: <strong>ADAGrad optimizer</strong></p><ul><li><code>solve(problem, ADAGrad(η))</code></li><li><code>η</code> is the learning rate</li><li>Defaults:<ul><li><code>η = 0.1</code></li></ul></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.ADADelta"><code>Flux.Optimise.ADADelta</code></a>: <strong>ADADelta optimizer</strong></p><ul><li><code>solve(problem, ADADelta(ρ))</code></li><li><code>ρ</code> is the gradient decay factor</li><li>Defaults:<ul><li><code>ρ = 0.9</code></li></ul></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.ADAGrad"><code>Flux.Optimise.AMSGrad</code></a>: <strong>AMSGrad optimizer</strong></p><ul><li><code>solve(problem, AMSGrad(η, β::Tuple))</code></li><li><code>η</code> is the learning rate</li><li><code>β::Tuple</code> is the decay of momentums</li><li>Defaults:<ul><li><code>η = 0.001</code></li><li><code>β::Tuple = (0.9, 0.999)</code></li></ul></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.NADAM"><code>Flux.Optimise.NADAM</code></a>: <strong>Nesterov variant of the ADAM optimizer</strong></p><ul><li><code>solve(problem, NADAM(η, β::Tuple))</code></li><li><code>η</code> is the learning rate</li><li><code>β::Tuple</code> is the decay of momentums</li><li>Defaults:<ul><li><code>η = 0.001</code></li><li><code>β::Tuple = (0.9, 0.999)</code></li></ul></li></ul></li><li><p><a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.ADAMW"><code>Flux.Optimise.ADAMW</code></a>: <strong>ADAMW optimizer</strong></p><ul><li><code>solve(problem, ADAMW(η, β::Tuple))</code></li><li><code>η</code> is the learning rate</li><li><code>β::Tuple</code> is the decay of momentums</li><li><code>decay</code> is the decay to weights</li><li>Defaults:<ul><li><code>η = 0.001</code></li><li><code>β::Tuple = (0.9, 0.999)</code></li><li><code>decay = 0</code></li></ul></li></ul></li></ul><h2 id="Optim.jl"><a class="docs-heading-anchor" href="#Optim.jl">Optim.jl</a><a id="Optim.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Optim.jl" title="Permalink"></a></h2><ul><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/cg/"><code>Optim.ConjugateGradient</code></a>: <strong>Conjugate Gradient Descent</strong></p><ul><li><code>solve(problem, ConjugateGradient(alphaguess, linesearch, eta, P, precondprep))</code></li><li><code>alphaguess</code> computes the initial step length (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_initialstep.html">this example</a>)<ul><li>available initial step length procedures:</li><li><code>InitialPrevious</code></li><li><code>InitialStatic</code></li><li><code>InitialHagerZhang</code></li><li><code>InitialQuadratic</code></li><li><code>InitialConstantChange</code></li></ul></li><li><code>linesearch</code> specifies the line search algorithm (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_linesearch.html">this example</a>)<ul><li>available line search algorithms:</li><li><code>HaegerZhang</code></li><li><code>MoreThuente</code></li><li><code>BackTracking</code></li><li><code>StrongWolfe</code></li><li><code>Static</code></li></ul></li><li><code>eta</code> determines the next step direction</li><li><code>P</code> is an optional preconditioner (for more information, see <a href="https://julianlsolvers.github.io/Optim.jl/v0.9.3/algo/precondition/">this source</a>)</li><li><code>precondpred</code> is used to update <code>P</code> as the state variable <code>x</code> changes</li><li>Defaults:<ul><li><code>alphaguess = LineSearches.InitialHagerZhang()</code></li><li><code>linesearch = LineSearches.HagerZhang()</code></li><li><code>eta = 0.4</code></li><li><code>P = nothing</code></li><li><code>precondprep = (P, x) -&gt; nothing</code></li></ul></li></ul></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/gradientdescent/"><code>Optim.GradientDescent</code></a>: <strong>Gradient Descent (a quasi-Newton solver)</strong></p><ul><li><code>solve(problem, GradientDescent(alphaguess, linesearch, P, precondprep))</code></li><li><code>alphaguess</code> computes the initial step length (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_initialstep.html">this example</a>)<ul><li>available initial step length procedures:</li><li><code>InitialPrevious</code></li><li><code>InitialStatic</code></li><li><code>InitialHagerZhang</code></li><li><code>InitialQuadratic</code></li><li><code>InitialConstantChange</code></li></ul></li><li><code>linesearch</code> specifies the line search algorithm (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_linesearch.html">this example</a>)<ul><li>available line search algorithms:</li><li><code>HaegerZhang</code></li><li><code>MoreThuente</code></li><li><code>BackTracking</code></li><li><code>StrongWolfe</code></li><li><code>Static</code></li></ul></li><li><code>P</code> is an optional preconditioner (for more information, see <a href="https://julianlsolvers.github.io/Optim.jl/v0.9.3/algo/precondition/">this source</a>)</li><li><code>precondpred</code> is used to update <code>P</code> as the state variable <code>x</code> changes</li><li>Defaults:<ul><li><code>alphaguess = LineSearches.InitialPrevious()</code></li><li><code>linesearch = LineSearches.HagerZhang()</code></li><li><code>P = nothing</code></li><li><code>precondprep = (P, x) -&gt; nothing</code></li></ul></li></ul></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/lbfgs/"><code>Optim.BFGS</code></a>: <strong>Broyden-Fletcher-Goldfarb-Shanno algorithm</strong></p><ul><li><code>solve(problem, BFGS(alpaguess, linesearch, initial_invH, initial_stepnorm, manifold))</code></li><li><code>alphaguess</code> computes the initial step length (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_initialstep.html">this example</a>)<ul><li>available initial step length procedures:</li><li><code>InitialPrevious</code></li><li><code>InitialStatic</code></li><li><code>InitialHagerZhang</code></li><li><code>InitialQuadratic</code></li><li><code>InitialConstantChange</code></li></ul></li><li><code>linesearch</code> specifies the line search algorithm (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_linesearch.html">this example</a>)<ul><li>available line search algorithms:</li><li><code>HaegerZhang</code></li><li><code>MoreThuente</code></li><li><code>BackTracking</code></li><li><code>StrongWolfe</code></li><li><code>Static</code></li></ul></li><li><code>initial_invH</code> specifies an optional initial matrix</li><li><code>initial_stepnorm</code> determines that <code>initial_invH</code> is an identity matrix scaled by the value of <code>initial_stepnorm</code> multiplied by the sup-norm of the gradient at the initial point</li><li><code>manifold</code> specifies a (Riemannian) manifold on which the function is to be minimized (for more information, consult <a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/manifolds/">this source</a>)<ul><li>available manifolds:</li><li><code>Flat</code></li><li><code>Sphere</code></li><li><code>Stiefel</code></li><li>meta-manifolds:</li><li><code>PowerManifold</code></li><li><code>ProductManifold</code></li><li>custom manifolds</li></ul></li><li>Defaults:<ul><li><code>alphaguess = LineSearches.InitialStatic()</code></li><li><code>linesearch = LineSearches.HagerZhang()</code></li><li><code>initial_invH = nothing</code></li><li><code>initial_stepnorm = nothing</code></li><li><code>manifold = Flat()</code></li></ul></li></ul></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/lbfgs/"><code>Optim.LBFGS</code></a>: <strong>Limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm</strong></p><ul><li><code>m</code> is the number of history points</li><li><code>alphaguess</code> computes the initial step length (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_initialstep.html">this example</a>)<ul><li>available initial step length procedures:</li><li><code>InitialPrevious</code></li><li><code>InitialStatic</code></li><li><code>InitialHagerZhang</code></li><li><code>InitialQuadratic</code></li><li><code>InitialConstantChange</code></li></ul></li><li><code>linesearch</code> specifies the line search algorithm (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_linesearch.html">this example</a>)<ul><li>available line search algorithms:</li><li><code>HaegerZhang</code></li><li><code>MoreThuente</code></li><li><code>BackTracking</code></li><li><code>StrongWolfe</code></li><li><code>Static</code></li></ul></li><li><code>P</code> is an optional preconditioner (for more information, see <a href="https://julianlsolvers.github.io/Optim.jl/v0.9.3/algo/precondition/">this source</a>)</li><li><code>precondpred</code> is used to update <code>P</code> as the state variable <code>x</code> changes</li><li><code>manifold</code> specifies a (Riemannian) manifold on which the function is to be minimized (for more information, consult <a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/manifolds/">this source</a>)<ul><li>available manifolds:</li><li><code>Flat</code></li><li><code>Sphere</code></li><li><code>Stiefel</code></li><li>meta-manifolds:</li><li><code>PowerManifold</code></li><li><code>ProductManifold</code></li><li>custom manifolds</li></ul></li><li><code>scaleinvH0</code>: whether to scale the initial Hessian approximation</li><li>Defaults:<ul><li><code>m = 10</code></li><li><code>alphaguess = LineSearches.InitialStatic()</code></li><li><code>linesearch = LineSearches.HagerZhang()</code></li><li><code>P = nothing</code></li><li><code>precondprep = (P, x) -&gt; nothing</code></li><li><code>manifold = Flat()</code></li><li><code>scaleinvH0::Bool = true &amp;&amp; (typeof(P) &lt;: Nothing)</code></li></ul></li></ul></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/ngmres/"><code>Optim.NGMRES</code></a></p></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/ngmres/"><code>Optim.OACCEL</code></a></p></li></ul><h3 id="Optim-Keyword-Arguments"><a class="docs-heading-anchor" href="#Optim-Keyword-Arguments">Optim Keyword Arguments</a><a id="Optim-Keyword-Arguments-1"></a><a class="docs-heading-anchor-permalink" href="#Optim-Keyword-Arguments" title="Permalink"></a></h3><p>The following special keyword arguments can be used with Optim.jl optimizers:</p><ul><li><code>x_tol</code>: Absolute tolerance in changes of the input vector <code>x</code>, in infinity norm. Defaults to <code>0.0</code>.</li><li><code>f_tol</code>: Relative tolerance in changes of the objective value. Defaults to <code>0.0</code>.</li><li><code>g_tol</code>: Absolute tolerance in the gradient, in infinity norm. Defaults to <code>1e-8</code>. For gradient free methods, this will control the main convergence tolerance, which is solver specific.</li><li><code>f_calls_limit</code>: A soft upper limit on the number of objective calls. Defaults to <code>0</code> (unlimited).</li><li><code>g_calls_limit</code>: A soft upper limit on the number of gradient calls. Defaults to <code>0</code> (unlimited).</li><li><code>h_calls_limit</code>: A soft upper limit on the number of Hessian calls. Defaults to <code>0</code> (unlimited).</li><li><code>allow_f_increases</code>: Allow steps that increase the objective value. Defaults to <code>false</code>. Note that, when setting this to <code>true</code>, the last iterate will be returned as the minimizer even if the objective increased.</li><li><code>iterations</code>: How many iterations will run before the algorithm gives up? Defaults to <code>1_000</code>.</li><li><code>store_trace</code>: Should a trace of the optimization algorithm&#39;s state be stored? Defaults to <code>false</code>.</li><li><code>show_trace</code>: Should a trace of the optimization algorithm&#39;s state be shown on <code>stdout</code>? Defaults to <code>false</code>.</li><li><code>extended_trace</code>: Save additional information. Solver dependent. Defaults to <code>false</code>.</li><li><code>trace_simplex</code>: Include the full simplex in the trace for <code>NelderMead</code>. Defaults to <code>false</code>.</li><li><code>show_every</code>: Trace output is printed every <code>show_every</code>th iteration.</li><li><code>time_limit</code>: A soft upper limit on the total run time. Defaults to <code>NaN</code> (unlimited).</li></ul><h2 id="Ipopt.jl-(MathOptInterface)"><a class="docs-heading-anchor" href="#Ipopt.jl-(MathOptInterface)">Ipopt.jl (MathOptInterface)</a><a id="Ipopt.jl-(MathOptInterface)-1"></a><a class="docs-heading-anchor-permalink" href="#Ipopt.jl-(MathOptInterface)" title="Permalink"></a></h2><ul><li><a href="https://juliahub.com/docs/Ipopt/yMQMo/0.7.0/"><code>Ipopt.Optimizer</code></a></li><li>Ipopt is a MathOptInterface optimizer, and thus its options are handled via <code>GalacticOptim.MOI.OptimizerWithAttributes(Ipopt.Optimizer, &quot;option_name&quot; =&gt; option_value, ...)</code></li><li>The full list of optimizer options can be found in the <a href="https://coin-or.github.io/Ipopt/OPTIONS.html#OPTIONS_REF">Ipopt Documentation</a></li></ul><h2 id="NLopt.jl"><a class="docs-heading-anchor" href="#NLopt.jl">NLopt.jl</a><a id="NLopt.jl-1"></a><a class="docs-heading-anchor-permalink" href="#NLopt.jl" title="Permalink"></a></h2><p>NLopt.jl algorithms are chosen via <code>NLopt.Opt(:algname)</code>. Consult the <a href="https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/">NLopt Documentation</a> for more information on the algorithms. Possible algorithm names are:</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../optimization_packages/quaddirect/">« QuadDIRECT.jl</a><a class="docs-footer-nextpage" href="../local_derivative_free/">Local Derivative-Free Optimization »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.7 on <span class="colophon-date" title="Monday 4 October 2021 14:38">Monday 4 October 2021</span>. Using Julia version 1.6.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
